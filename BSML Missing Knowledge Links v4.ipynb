{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a486ea6",
   "metadata": {},
   "source": [
    "# BSML Project - Missing Knowledge Links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b302962",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d7111cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import mwclient\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.cluster import KMeans\n",
    "import scipy.sparse as sp\n",
    "from functools import lru_cache\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13861cb7",
   "metadata": {},
   "source": [
    "### Part 1: given a link, find all connected links up to a certain depth, and create the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f88d78b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function just returns a list of all links directly connected to the input link\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def get_links(title):\n",
    "    page = wikipedia_site.pages[title]\n",
    "    links = [link.name for link in page.links()]\n",
    "    time.sleep(1)  # da capire se necessario\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45c29d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph_with_links(graph, starting_title, max_depth=1, visited_nodes=None):\n",
    "    \"\"\"\n",
    "    Adds links to a graph starting from a given title.\n",
    "\n",
    "    Parameters:\n",
    "    - graph: NetworkX Graph object\n",
    "    - starting_title: str, the starting title for link exploration\n",
    "    - max_depth: int, the maximum depth to explore links (default is 1)\n",
    "    - visited_nodes: set, a set to keep track of visited nodes (default is None)\n",
    "    \"\"\"\n",
    "    if visited_nodes is None:\n",
    "        visited_nodes = set()\n",
    "\n",
    "    # We decided to filter out certain links\n",
    "    link_filter_keywords = ['Wikipedia', 'Category', 'identifier', 'Help', 'Template', ':']\n",
    "\n",
    "    # This is the base case: if depth is 0 or the title has already been visited, return\n",
    "    if max_depth == 0 or starting_title in visited_nodes:\n",
    "        return\n",
    "\n",
    "    # The current title is marked as visited\n",
    "    visited_nodes.add(starting_title)\n",
    "\n",
    "    # Get links for the current title\n",
    "    links = get_links(starting_title)\n",
    "\n",
    "    # By iterating through links, we actually build the graph\n",
    "    for link in links:\n",
    "        # Check if the link contains any filter keywords\n",
    "        if  not any(keyword in link for keyword in link_filter_keywords):\n",
    "            # If the link is not already in the graph, add it and explore further\n",
    "            if link not in graph:\n",
    "                graph.add_edge(starting_title, link)\n",
    "                build_graph_with_links(graph, link, max_depth-1, visited_nodes)\n",
    "            else:\n",
    "                # If the link is already in the graph, just add the edge\n",
    "                graph.add_edge(starting_title, link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efd0f8d",
   "metadata": {},
   "source": [
    "### Part 2: Perform Clustering Analysis on Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bfca7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With this function the aim is to start from an adjacency matrix (which represents the graph\n",
    "# created with the function above), store it in a more memory-efficient format, and transform\n",
    "# it into a matrix in which nodes that are not directly connected, but are anyway \"near to each\n",
    "# other\" in the original graph, are now connected (the smaller the distance between two nodes\n",
    "# in the original graph, the stronger the link between them that we create here). We also make \n",
    "# sure the resulting matrix stays symmetric. This new matrix allow us to later perform \n",
    "# some clustering\n",
    "\n",
    "\n",
    "def apply_depth(adjacency_matrix, additional_depth = 2):\n",
    "    \"\"\"\n",
    "    Applies depth-based transformations to an adjacency matrix and normalizes the result.\n",
    "\n",
    "    Parameters:\n",
    "    - adjacency_matrix: 2D array-like, the original adjacency matrix\n",
    "    - depth: int, the depth of transformations to apply\n",
    "\n",
    "    Returns:\n",
    "    - 2D array, the transformed and normalized adjacency matrix\n",
    "    \"\"\"\n",
    "    # Convert the adjacency matrix to a Compressed Sparse Row (CSR) matrix\n",
    "    adj_matrix = sp.csr_matrix(adjacency_matrix)\n",
    "\n",
    "    if additional_depth == 0:\n",
    "        return adj_matrix.toarray() #return it as an array\n",
    "\n",
    "    # Now we apply a series of depth-based transformations to the adjacency matrix:\n",
    "    # each iteration updates the adjacency matrix by adding a fraction of\n",
    "    # the matrix multiplied by itself (adj_matrix @ adj_matrix).\n",
    "    # The fraction is determined by the term 1 / (2 ** (i + 1)), which\n",
    "    # decreases with each iteration, effectively diminishing the influence\n",
    "    # of higher powers in the matrix multiplication.\n",
    "    # This process creates weaker links between nodes that aren't connected\n",
    "    #Â at depth=1 but are connected at a higher depth. The higher the depth,\n",
    "    # the lower the strength of their new connection.\n",
    "\n",
    "    for i in range(0, additional_depth):\n",
    "        adj_matrix = adj_matrix + 1 / (2 ** (i+1)) * adj_matrix @ adj_matrix\n",
    "\n",
    "    # Here we compute the transpose of the adjacency matrix and we sum the original matrix \n",
    "    # with its transpose to obtain a symmetric matrix\n",
    "    adj_matrix_transpose = adj_matrix.transpose()\n",
    "    symmetric_matrix = adj_matrix + adj_matrix_transpose\n",
    "    \n",
    "    # Here we normalize the matrix by setting the diagonal values to a scaled maximum value\n",
    "    max_value = adj_matrix.max()\n",
    "    normalized_matrix = 0.9 * (symmetric_matrix / max_value)\n",
    "    normalized_matrix.setdiag(1)\n",
    "\n",
    "    # The algorithm returns the normalized transformed matrix as an array\n",
    "    return normalized_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e29e1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_affinity_propagation_clustering(similarity_matrix):\n",
    "    \"\"\"\n",
    "    Clusters nodes in a graph based on the given similarity matrix using Affinity Propagation.\n",
    "\n",
    "    Parameters:\n",
    "    - similarity_matrix: 2D array-like, a matrix representing pairwise similarities between nodes\n",
    "\n",
    "    Returns:\n",
    "    - 1D array, representing cluster assignments for each node\n",
    "    \"\"\"\n",
    "    \n",
    "    affinity_propagation_model = AffinityPropagation(affinity='precomputed', damping=0.98)\n",
    "\n",
    "    # This line performs the actual clustering\n",
    "    cluster_assignments = affinity_propagation_model.fit_predict(similarity_matrix)\n",
    "    \n",
    "    return np.array(cluster_assignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d372fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a more efficient way to perform the clustering, although slighty less accurate than \n",
    "# the previous one\n",
    "\n",
    "def perform_kmeans_clustering(similarity_matrix, num_clusters):\n",
    "    \"\"\"\n",
    "    Clusters nodes in a graph based on the given similarity matrix using K-Means.\n",
    "\n",
    "    Parameters:\n",
    "    - similarity_matrix: 2D array-like, a matrix representing pairwise similarities between nodes\n",
    "    - num_clusters: int, the number of clusters to form\n",
    "\n",
    "    Returns:\n",
    "    - 1D array, representing cluster assignments for each node\n",
    "    \"\"\"\n",
    "    \n",
    "    distance_matrix = 1 - similarity_matrix # this is a simple way to convert a similarity \n",
    "                                            # matrix to a distance one (which is what we need \n",
    "                                            # for K-Means)\n",
    "\n",
    "    kmeans_model = KMeans(n_clusters=num_clusters, random_state=0)\n",
    "\n",
    "    # This line performs the actual clustering\n",
    "    cluster_assignments = kmeans_model.fit_predict(distance_matrix)\n",
    "\n",
    "    return np.array(cluster_assignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f93462f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the well-known Elbow method to estimate the optimal number of clusters\n",
    "\n",
    "def elbow_method(similarity_matrix, min_k=10, max_k=100, step=10):\n",
    "    \"\"\"\n",
    "    Estimates the optimal number of clusters for K-Means clustering using the Elbow Method.\n",
    "\n",
    "    Parameters:\n",
    "    - similarity_matrix: 2D array-like, a matrix representing pairwise similarities between nodes\n",
    "    - max_k: int, the minimum number of clusters to consider\n",
    "    - max_k: int, the maximum number of clusters to consider\n",
    "\n",
    "    Returns:\n",
    "    - int, an estimated optimal number of clusters\n",
    "    \"\"\"\n",
    "    \n",
    "    distance_matrix = 1 - similarity_matrix\n",
    "    inertia_values = []\n",
    "\n",
    "    for k in range(min_k, max_k+1, step):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "        kmeans.fit(distance_matrix)\n",
    "        inertia_values.append(kmeans.inertia_)\n",
    "        \n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(range(min_k, max_k+1, step), inertia_values, marker='o')\n",
    "    plt.title('Elbow Method For Optimal k')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Inertia')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1d2cc6",
   "metadata": {},
   "source": [
    "### Part 3 - Find Potential Missing Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c83b2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_missing_links(graph, similarity_matrix, clusters_array):\n",
    "    \"\"\"\n",
    "    Identifies missing links between nodes in the same cluster based on the given graph,\n",
    "    similarity matrix, and cluster assignments.\n",
    "\n",
    "    Parameters:\n",
    "    - graph: NetworkX Graph object, the input graph\n",
    "    - similarity_matrix: 2D array-like, a matrix representing pairwise similarities between nodes\n",
    "    - clusters: 1D array-like, representing cluster assignments for each node\n",
    "\n",
    "    Returns:\n",
    "    - dict, a dictionary of missing links and their associated similarity scores\n",
    "    \"\"\"\n",
    "    # We create an array of node indices\n",
    "    nodes_list = list(graph.nodes)\n",
    "    nodes_indices = np.arange(len(nodes_list))\n",
    "\n",
    "    max_cluster_index = np.max(clusters_array)\n",
    "    adjacency_matrix = np.array(nx.adjacency_matrix(graph).todense())\n",
    "    missing_links = {} # the keys are the potentially missing links, while the \n",
    "                       # values are their similarity scores\n",
    "\n",
    "    # Iterate through each cluster\n",
    "    for cluster_index in range(max_cluster_index + 1):\n",
    "        cluster_mask = (clusters_array == cluster_index)\n",
    "        nodes_in_cluster = nodes_indices[cluster_mask]\n",
    "        # We iterate through pairs of nodes in the current cluster\n",
    "        for i1, node1 in enumerate(nodes_in_cluster):\n",
    "            for node2 in nodes_in_cluster[i1+1:]:\n",
    "                # There must be no edge between the nodes in the graph\n",
    "                if adjacency_matrix[node1, node2] == 0 and adjacency_matrix[node2, node1] == 0:\n",
    "                    # There must be a positive similarity score in the similarity matrix\n",
    "                    if similarity_matrix[node1, node2] > 0:\n",
    "                        \n",
    "                        first_node, second_node = nodes_list[node1], nodes_list[node2]\n",
    "                        missing_links[(first_node, second_node)] = np.round(similarity_matrix[node1, node2], 2)\n",
    "                        \n",
    "    return missing_links # we return the dictionary of missing links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5bb5613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_links(link_scores, n):\n",
    "    \"\"\"\n",
    "    Retrieves the top n links based on their scores from a dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    - link_scores: dict, a dictionary where keys are link identifiers and values are scores\n",
    "    - n: int, the number of top links to retrieve\n",
    "\n",
    "    Returns:\n",
    "    - dict, a new dictionary containing the top n links and their scores\n",
    "    \"\"\"\n",
    "    \n",
    "    sorted_link_scores = sorted(link_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_n_items = sorted_link_scores[:n]\n",
    "    top_n_links_dict = dict(top_n_items)\n",
    "    return top_n_links_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6734db0f",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3f41a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_site = mwclient.Site('en.wikipedia.org', path='/w/')\n",
    "wikipedia_site.connection.headers.update({'User-Agent': 'WikipediaNetworkResearch (er.pupone@gmail.com)'})\n",
    "\n",
    "# Initialize a directed graph for storing Wikipedia network information\n",
    "wikipedia_graph = nx.DiGraph()\n",
    "\n",
    "# Examples of an interesting starting page with many links (these are pages with plenty of links)\n",
    "#start_node_fourier_transform = \"Fourier transform\" # (approximately 4 min to build graph)\n",
    "start_node_cure_of_cancer = \"Cancer treatment\" # (approximately 2 min and 10s to build graph)\n",
    "\n",
    "# Examples of fun starting nodes\n",
    "#start_node_bocconi = \"Bocconi University\"\n",
    "#start_node_acmilan = \"AC Milan\"\n",
    "\n",
    "build_graph_with_links(wikipedia_graph, start_node_cure_of_cancer, max_depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "793ce9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes in the graph: 12145\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of nodes in the graph:\", len(wikipedia_graph.nodes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65bd5120",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Giulio/opt/anaconda3/lib/python3.9/site-packages/scipy/sparse/_index.py:125: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    }
   ],
   "source": [
    "adjacency_matrix = nx.adjacency_matrix(wikipedia_graph)\n",
    "transformed_matrix = apply_depth(adjacency_matrix, additional_depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "577cd527",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Giulio/opt/anaconda3/lib/python3.9/site-packages/sklearn/cluster/_affinity_propagation.py:148: FutureWarning: 'random_state' has been introduced in 0.23. It will be set to None starting from 1.0 (renaming of 0.25) which means that results will differ at every function call. Set 'random_state' to None to silence this warning, or to 0 to keep the behavior of versions <0.23.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# This is the clustering performed with Affinity Propagation. \n",
    "# After careful consideration, we opted for this method over K-Means\n",
    "clusters = perform_affinity_propagation_clustering(transformed_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2da6dfad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters:  34\n",
      "Sizes of clusters:  [4369, 23, 244, 36, 31, 38, 51, 222, 502, 355, 70, 763, 875, 110, 73, 191, 1570, 85, 60, 29, 36, 109, 1026, 62, 274, 59, 170, 136, 176, 20, 50, 196, 42, 92]\n"
     ]
    }
   ],
   "source": [
    "num_clusters = np.max(clusters)+1    # only for clusters created with Propagation Clustering\n",
    "print('Number of clusters: ', num_clusters)\n",
    "\n",
    "sizes = []\n",
    "for i in range(num_clusters):\n",
    "    mask = (clusters == i)\n",
    "    cluster_size = np.sum(mask)\n",
    "    sizes.append(cluster_size)\n",
    "print('Sizes of clusters: ', sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d51d8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_links = find_missing_links(wikipedia_graph, transformed_matrix, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84f4f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(missing_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6f84e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get the top 20 missing links based on their scores\n",
    "top_missing_links = get_top_n_links(missing_links, n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8b93629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 missing links:\n",
      "Cancer treatment <-0.59-> Breast cancer\n",
      "Cancer treatment <-0.56-> Medical Subject Headings\n",
      "Cancer treatment <-0.39-> Apoptosis\n",
      "Cancer treatment <-0.32-> Gene\n",
      "Cancer treatment <-0.31-> Protein Data Bank\n",
      "Cancer treatment <-0.31-> Mutation\n",
      "Cancer treatment <-0.3-> Neoplasm\n",
      "Cancer treatment <-0.3-> Protein\n",
      "Cancer treatment <-0.3-> DNA repair\n",
      "Cancer treatment <-0.29-> Oncogene\n",
      "Cancer treatment <-0.29-> P53\n",
      "Cancer treatment <-0.28-> Inflammation\n",
      "Cancer treatment <-0.28-> Ensembl\n",
      "Cancer treatment <-0.28-> Entrez\n",
      "Cancer treatment <-0.28-> GeneCards\n",
      "Cancer treatment <-0.28-> Gene expression\n",
      "Cancer treatment <-0.28-> Gene nomenclature\n",
      "Cancer treatment <-0.28-> Orthologs\n",
      "Cancer treatment <-0.28-> PubMed\n",
      "Cancer treatment <-0.28-> UniProt\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 20 missing links:\")\n",
    "for nodes, link_strength in top_missing_links.items():\n",
    "    node1, node2 = nodes\n",
    "    print(f'{node1} <-{link_strength}-> {node2}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
